ğŸ“„ **Technical Report**: [Neuro-Symbolic Memory Network â€“ PDF](./Technical_Report.pdf)

---
# ğŸŒŸ NeuroSymbolic Memory Network (NSMN)
### Explainable Multi-Hop Reasoning with Neural Embeddings + Symbolic Graph Inference

The NeuroSymbolic Memory Network (NSMN) is a hybrid reasoning system that combines neural semantic retrieval with symbolic graph-based inference. It generates transparent, multi-hop explanations from natural-language facts, providing interpretable reasoning instead of black-box answers.

---

## ğŸ§  Key Features

### ğŸ”¹ Triple Extraction
Natural language facts are converted into structured triples:

    (subject) â€” (relation) â€” (object)

using a curated set of linguistic patterns capturing:
causation, transformation, production, essentiality, requirements, partâ€“whole, support, usage, identity, and association.

---

### ğŸ”¹ Neural Semantic Retrieval
Facts are embedded using Sentence Transformers (MiniLM).  
Queries retrieve relevant facts using cosine similarity, enabling semantic matching even with different wording.

---

### ğŸ”¹ Symbolic Knowledge Graph
Extracted triples form a directed graph:

    A  --relation-->  B

Using networkx, the system performs:
- multi-hop inference
- causal/structural chain discovery
- logical reasoning across facts

---

### ğŸ”¹ Multi-Hop Chain Composition
Given a question, NSMN:

1. retrieves relevant facts using embeddings  
2. explores symbolic graph paths  
3. composes multi-step reasoning chains  
4. scores each chain using semantic + structural criteria  
5. returns a clear, interpretable explanation  

All reasoning steps are fully visible.

---

### ğŸ”¹ Optional LLM Modules
LLMs are optional (disabled by default). When enabled, they can:
- provide natural-language rationales  
- suggest additional bridging facts  

The core reasoning remains symbolic and explainable.

---

## ğŸ® Interactive App

A Streamlit user interface supports:

- adding new facts  
- asking questions  
- viewing reasoning chains  
- inspecting retrieved facts  
- exploring memory  
- clearing/resetting the knowledge base  

Run locally with:

    streamlit run app.py

---

# ğŸ” Examples

Below are fully working examples you can paste directly into the app.

---

## âœ… Example 1 â€” Leaves â†’ Sunlight Reasoning

### Facts:

    Leaves are part of a plant
    Leaves support photosynthesis
    Photosynthesis requires sunlight

### Query:

    How are leaves connected to sunlight?

---

## âœ… Example 2 â€” Ice â†’ Turbine Rotation Reasoning

### Facts:

    Ice turns into water
    Water turns into steam
    Steam causes pressure increase
    Pressure increase leads to turbine rotation

### Query:

    How does ice lead to turbine rotation?

---

# ğŸ§± Architecture
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚     Natural Language Input    â”‚
                          â”‚ (User facts & questions)      â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚       Triple Extraction       â”‚
                          â”‚ (pattern-based NLP â†’ triples) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                 Symbolic Memory Store                  â”‚
              â”‚  - stores triples (subj, pred, obj)                    â”‚
              â”‚  - creates neural embeddings (SentenceTransformer)     â”‚
              â”‚  - supports cosine similarity retrieval                â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                               â”‚
                              â–¼                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      Knowledge Graph         â”‚   â”‚        Neural Retrieval       â”‚
              â”‚ (networkx DiGraph of triples)â”‚   â”‚ (bruteforce cosine search)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                                 â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚        Chain Composer         â”‚
                          â”‚  - local composition          â”‚
                          â”‚    (object â†’ subject links)   â”‚
                          â”‚  - graph multi-hop paths      â”‚
                          â”‚  - generates reasoning chains â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚     Hybrid Scoring Engine     â”‚
                          â”‚  - semantic similarity        â”‚
                          â”‚  - lexical overlap            â”‚
                          â”‚  - chain depth bonus          â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Final Explainable Output    â”‚
                          â”‚ (step-by-step reasoning chain)â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

# ğŸ› Why This Project Matters

NSMN demonstrates:

- hybrid neuralâ€“symbolic architecture  
- interpretable, step-by-step reasoning  
- multi-hop logical chains  
- semantic retrieval with embeddings  
- graph-based inference  
- optional LLM enhancement without dependency  
- a clean deployment via Streamlit  

---

# ğŸ”® Future Directions

- interactive knowledge-graph visualization  
- FAISS / ScaNN accelerated retrieval  
- transformer-based relation extraction  
- differentiable reasoning modules  
- formal evaluations on chain-depth tasks  
- integration with retrieval-augmented LLMs  

---

# ğŸ“„ Citation

    Agrit Mishra. "NeuroSymbolic Memory Network: 
    Explainable Multi-Hop Reasoning with Hybrid Neural-Symbolic Architecture." 2025.

---

# ğŸ“ License

MIT License.

